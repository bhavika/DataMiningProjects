/home/bhavika/.local/lib/python2.7/site-packages/imblearn/under_sampling/nearmiss.py:161: UserWarning: The number of the samples to be selected is larger than the number of samples available. The balancing ratio cannot be ensure and all samples will be returned.
  warnings.warn('The number of the samples to be selected is larger than'
No of features:  2115
Time taken to fit VarianceThreshold:  0.000173091888428
------------
RandomUnderSampler(random_state=None, ratio='auto', replacement=True,
          return_indices=False) 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.90312499999999996)
('auc', 0.81415336533095217)

             precision    recall  f1-score   support

          0       0.94      0.95      0.95       289
          1       0.50      0.48      0.49        31

avg / total       0.90      0.90      0.90       320


-----------------
------------
TomekLinks(n_jobs=-1, random_state=None, return_indices=False) 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.94062500000000004)
('auc', 0.91025784127692821)

             precision    recall  f1-score   support

          0       0.94      1.00      0.97       289
          1       0.93      0.42      0.58        31

avg / total       0.94      0.94      0.93       320


-----------------
------------
ClusterCentroids(n_jobs=-1, random_state=None, ratio='auto') 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.88437500000000002)
('auc', 0.86471704431298124)

             precision    recall  f1-score   support

          0       0.96      0.91      0.93       289
          1       0.44      0.68      0.53        31

avg / total       0.91      0.88      0.90       320


-----------------
------------
NearMiss(n_jobs=-1, random_state=None, ratio='auto', return_indices=False,
     size_ngh=5, ver3_samp_ngh=3, version=1) 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.90625)
('auc', 0.83128697399263307)

             precision    recall  f1-score   support

          0       0.96      0.94      0.95       289
          1       0.51      0.61      0.56        31

avg / total       0.91      0.91      0.91       320


-----------------
------------
NearMiss(n_jobs=-1, random_state=None, ratio='auto', return_indices=False,
     size_ngh=7, ver3_samp_ngh=3, version=2) 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.89375000000000004)
('auc', 0.83346355620046886)

             precision    recall  f1-score   support

          0       0.96      0.92      0.94       289
          1       0.47      0.65      0.54        31

avg / total       0.91      0.89      0.90       320


-----------------
------------
NearMiss(n_jobs=-1, random_state=None, ratio='auto', return_indices=False,
     size_ngh=3, ver3_samp_ngh=3, version=3) 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.87187499999999996)
('auc', 0.75784127692822856)

             precision    recall  f1-score   support

          0       0.95      0.91      0.93       289
          1       0.39      0.55      0.45        31

avg / total       0.89      0.87      0.88       320


-----------------
------------
CondensedNearestNeighbour(n_jobs=-1, n_seeds_S=51, random_state=None,
             return_indices=False, size_ngh=3) 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.9375)
('auc', 0.84596495144547379)

             precision    recall  f1-score   support

          0       0.96      0.98      0.97       289
          1       0.72      0.58      0.64        31

avg / total       0.93      0.94      0.93       320


-----------------
------------
OneSidedSelection(n_jobs=-1, n_seeds_S=51, random_state=None,
         return_indices=False, size_ngh=5) 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.92812499999999998)
('auc', 0.77955128920638472)

             precision    recall  f1-score   support

          0       0.93      0.99      0.96       289
          1       0.83      0.32      0.47        31

avg / total       0.92      0.93      0.91       320


-----------------
------------
OneSidedSelection(n_jobs=-1, n_seeds_S=35, random_state=None,
         return_indices=False, size_ngh=5) 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.94374999999999998)
('auc', 0.82006920415224915)

             precision    recall  f1-score   support

          0       0.95      0.99      0.97       289
          1       0.84      0.52      0.64        31

avg / total       0.94      0.94      0.94       320


-----------------
------------
InstanceHardnessThreshold(cv=5, estimator='linear-svm', n_jobs=-1,
             random_state=None, ratio='auto', return_indices=False) 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.62812500000000004)
('auc', 0.88547828998772193)

             precision    recall  f1-score   support

          0       0.98      0.60      0.75       289
          1       0.19      0.87      0.31        31

avg / total       0.90      0.63      0.70       320


-----------------
------------
RandomOverSampler(random_state=None, ratio='auto') 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.92812499999999998)
('auc', 0.827380287978569)

             precision    recall  f1-score   support

          0       0.94      0.98      0.96       289
          1       0.70      0.45      0.55        31

avg / total       0.92      0.93      0.92       320


-----------------
------------
ADASYN(k=3, n_jobs=1, random_state=None, ratio='auto') 
------------
-------------
SGDClassifier(alpha=0.07, average=False, class_weight=None, epsilon=0.1,
       eta0=0.0, fit_intercept=True, l1_ratio=0.15,
       learning_rate='optimal', loss='modified_huber', n_iter=10000,
       n_jobs=1, penalty='elasticnet', power_t=0.5, random_state=None,
       shuffle=True, verbose=0, warm_start=False)   
-----------------

('Accuracy score', 0.94062500000000004)
('auc', 0.83435651300368341)

             precision    recall  f1-score   support

          0       0.95      0.99      0.97       289
          1       0.80      0.52      0.63        31

avg / total       0.94      0.94      0.93       320


-----------------
Traceback (most recent call last):
  File "imbalances_sgd.py", line 73, in <module>
    sampling()
  File "imbalances_sgd.py", line 49, in sampling
    X_tr, y_tr = imbalance.fit_sample(X_tr, y_tr)
  File "/home/bhavika/.local/lib/python2.7/site-packages/imblearn/base.py", line 178, in fit_sample
    return self.fit(X, y).sample(X, y)
  File "/home/bhavika/.local/lib/python2.7/site-packages/imblearn/base.py", line 112, in fit
    raise RuntimeError('The ratio requested at initialisation'
RuntimeError: The ratio requested at initialisation should be greater or equal than the balancing ratio of the current data.
